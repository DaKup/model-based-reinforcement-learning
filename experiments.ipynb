{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1) Import Dependencies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboard\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ndim            = 2         # number of dimensions\n",
    "T               = 500       # maximum time steps and length of each trajectory\n",
    "M               = 250       # number of generated trajectories using pi_explore\n",
    "\n",
    "num_observables = 3         # number of observable targets from the current timestep\n",
    "\n",
    "a_max           = 100       # maximum acceleration\n",
    "d_max           = 3 * a_max # max distance between trajectory points\n",
    "v_max           = 5 * a_max # maximum velocity\n",
    "a_min           = -a_max    # minimum acceleration\n",
    "v_min           = -v_max    # minimum velocity\n",
    "d_min           = 1         # min distance between trajectory points"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Environment Dynamics\n",
    "\n",
    "$\\begin{aligned}\n",
    "v_{t+1} &= v_t + a_va_t\\\\\n",
    "x_{t+1} &= x_t + a_xv_t\\\\\n",
    "r_t &= -(x_t-\\mathbf{x^*})^T(x_t-\\mathbf{x^*})\n",
    "\\end{aligned}$\n",
    "\n",
    "with constants $a_v$ and $a_x$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "a_v = 2 # constant weight\n",
    "a_x = 2 # constant weight\n",
    "velocity_next   = lambda velocity, acceleration : velocity + a_v * acceleration\n",
    "position_next   = lambda position, velocity     : position + a_x * velocity\n",
    "get_reward      = lambda position, target       : -np.dot((position - target), np.transpose(position - target))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% toy environment dynamics\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Action Space\n",
    "\n",
    "Acceleration value for each dimension:\n",
    "\n",
    "$a_t \\in \\mathbb{R}^D$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "a = np.zeros(shape=ndim)        # current acceleration\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% action space\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5) State Space\n",
    "\n",
    "$s_t$ ...state consists of:\n",
    "\n",
    "- $x_t \\in \\mathbb{R}^D$ ...position vector\n",
    "- $v_t \\in \\mathbb{R}^D$ ...velocity vector\n",
    "- $\\mathbf{{x_t}^*} \\in \\mathbb{R}^D$ ...target vector(s)\n",
    "\n",
    "$s_t = (x_t, v_t, \\mathbf{{x_t}^*}, ..., \\mathbf{{x_{t+num\\_observables}}^*})$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "s = np.zeros(shape=(2+num_observables, ndim))   # current state\n",
    "x = s[0,:]                                      # position\n",
    "v = s[1,:]                                      # velocity\n",
    "y = s[2:,:]                                     # next observable target(s)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% state space\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "trajectories = np.zeros(shape=(M, T, ndim), dtype=np.float)\n",
    "for m in range(M):\n",
    "    for t in range(1, T):\n",
    "        trajectories[m, t] = trajectories[m, t-1] + np.random.randint(low=d_min, high=d_max, size=ndim, dtype='l')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% generate M random trajectories of length T\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "validation_trajectories = np.zeros(shape=(M, T, ndim), dtype=np.float)\n",
    "for m in range(M):\n",
    "    for t in range(1, T):\n",
    "        validation_trajectories[m, t] = validation_trajectories[m, t-1] + np.random.randint(low=d_min, high=d_max, size=ndim, dtype='l')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% generate validation trajectories\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "pi_explore = lambda state : np.random.randint(low=a_min, high=a_max, size=ndim)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% exploration policy (state) -> action\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "record_r = []\n",
    "record_a = []\n",
    "record_s = []\n",
    "\n",
    "num_episodes = 1 # observe each trajectory only once\n",
    "for _ in range(num_episodes):\n",
    "\n",
    "    # for each trajectory\n",
    "    # (or perhaps create mini batches?)\n",
    "    # (=> no minibatches, because trajectories may end at different time step?)\n",
    "    for m in range(M):\n",
    "\n",
    "        # reset agent's state:\n",
    "        # x[:] = np.zeros_like(x)\n",
    "        # v[:] = np.zeros_like(v)\n",
    "        s[:] = np.zeros_like(s)\n",
    "        # y[:] = trajectories[m,:num_observables]\n",
    "\n",
    "        # for each time step:\n",
    "        for t in range(T):\n",
    "\n",
    "            # get state (observable targets)\n",
    "            # tmp = min(num_observables, T - t)\n",
    "            # y[:tmp] = trajectories[m, t : min(t + num_observables, T)]\n",
    "            y[:min(num_observables, T - t)] = trajectories[m, t : min(t + num_observables, T)]\n",
    "\n",
    "            # get action\n",
    "            a = pi_explore(s)\n",
    "\n",
    "            # perform action (update state)\n",
    "            v[:] = velocity_next(v, a)\n",
    "            x[:] = position_next(x, v)\n",
    "\n",
    "            # get reward\n",
    "            r = get_reward(x, y[0])\n",
    "\n",
    "            # store observation\n",
    "            record_s.append(s.copy())\n",
    "            record_a.append(a.copy())\n",
    "            record_r.append(r.copy())\n",
    "\n",
    "record_s = np.array(record_s)\n",
    "record_a = np.array(record_a)\n",
    "record_r = np.array(record_r)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% observe environment dynamics using pi_explore\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# clone state:\n",
    "s_norm = record_s.copy()    # x, v, y: agent's position is always 0/0, target is normed to max_distance\n",
    "x_norm = s_norm[0,:]                      # position\n",
    "v_norm = s_norm[1,:]                      # velocity\n",
    "y_norm = s_norm[2:,:]                     # target(s)\n",
    "\n",
    "# normalize state:\n",
    "v_norm[:] = v_norm / v_max\n",
    "y_norm[:] = (y_norm - x_norm) / v_max\n",
    "x_norm[:] = np.zeros_like(x_norm)\n",
    "\n",
    "# normalize action:\n",
    "a_norm = record_a / a_max    # normed to maximum acceleration\n",
    "\n",
    "# normalize reward?:\n",
    "r_norm = record_r / v_max    # normed to max_distance or not normed at all?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% normalize data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "pi = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=s.reshape(-1).shape),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(32),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(a.size)#,\n",
    "    # keras.layers.Activation('tanh')\n",
    "])\n",
    "pi.compile(optimizer='adam', loss='mse')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% policy model (state) -> action\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "f_s = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=np.vstack([s, a]).reshape(-1).shape),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(32),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(s.size)#,\n",
    "    # keras.layers.Activation('tanh')\n",
    "])\n",
    "f_s.compile(optimizer='adam', loss='mse')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% state model (state, action) -> state\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "f_r = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=np.vstack([s, a]).reshape(-1).shape),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(32),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(1)#,\n",
    "    # keras.layers.Activation('tanh')\n",
    "])\n",
    "f_r.compile(optimizer='adam', loss='mse')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% reward model (state, action) -> reward\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "epochs = 250\n",
    "batch_size = 512\n",
    "state_action = np.concatenate([s_norm, np.expand_dims(a_norm, 1)], axis=1).reshape(s_norm.shape[0], -1)\n",
    "f_r.fit(state_action, r_norm, batch_size, epochs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% train reward model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "state_action2 = np.concatenate([s_norm, np.expand_dims(a_norm, 1)], axis=1).reshape(s_norm.shape[0], -1)[:-1]\n",
    "f_s.fit(state_action2, s_norm.reshape(s_norm.shape[0], -1)[1:], batch_size, epochs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% train state model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% train policy model using state and reward model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "pi.fit()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% train policy on environment model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-28-4157baeb9197>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0ms_norm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma_norm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mvstack\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;32mF:\\Anaconda\\envs\\ModelBasedRL\\lib\\site-packages\\numpy\\core\\shape_base.py\u001B[0m in \u001B[0;36mvstack\u001B[1;34m(tup)\u001B[0m\n\u001B[0;32m    281\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    282\u001B[0m         \u001B[0marrs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0marrs\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 283\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_nx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    284\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mconcatenate\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "np.vstack([s_norm, np.expand_dims(a_norm, 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% evaluate on real environment and repeat\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(125000, 1, 2)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(a_norm, 1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% evaluate on real environment and repeat\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(125000, 6, 2)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([s_norm, np.expand_dims(a_norm, 1)], axis=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.size\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-5fde42b4",
   "language": "python",
   "display_name": "PyCharm (Model-Based-Reinforcement-Learning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}