{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ndim                = 2             # number of dimensions\n",
    "T                   = 500           # maximum time steps and length of each trajectory\n",
    "M                   = 250           # number of generated trajectories using pi_explore\n",
    "\n",
    "a_max               = 100           # maximum acceleration\n",
    "a_min               = -a_max        # minimum acceleration\n",
    "\n",
    "d_max               = 3 * a_max     # max distance between trajectory points\n",
    "d_min               = 1             # min distance between trajectory points\n",
    "\n",
    "v_max               = 5 * a_max     # maximum velocity\n",
    "v_min               = -v_max        # minimum velocity\n",
    "\n",
    "num_observables             = 3             # number of observable targets from the current timestep\n",
    "max_agent_distance          = 100 * d_max   # maximum distance an agent may have to his next target before the episode is canceled\n",
    "max_distance_from_origin    = T * d_max     # use as normalization factor\n",
    "\n",
    "epochs              = 50\n",
    "batch_size          = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Toy Environment Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%% toy environment dynamics\n"
    }
   },
   "outputs": [],
   "source": [
    "a_v = 2.5 # constant weight\n",
    "a_x = 2.5 # constant weight\n",
    "velocity_next   = lambda velocity, acceleration : velocity + a_v * acceleration\n",
    "position_next   = lambda position, velocity     : position + a_x * velocity\n",
    "get_reward      = lambda position, target       : -np.linalg.norm(position - target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%% action space\n"
    }
   },
   "outputs": [],
   "source": [
    "a = np.zeros(shape=ndim)        # current acceleration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%% state space\n"
    }
   },
   "outputs": [],
   "source": [
    "s = np.zeros(shape=(2+num_observables, ndim))   # current state\n",
    "x = s[0,:]                                      # position\n",
    "v = s[1,:]                                      # velocity\n",
    "y = s[2:,:]                                     # next observable target(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "State Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%% state model\n"
    }
   },
   "outputs": [],
   "source": [
    "# option1: initial state + sequence of actions => return next state(s)\n",
    "# inputs = [batch, timesteps, feature]\n",
    "\n",
    "f_s_rnn = tf.keras.Sequential([\n",
    "      keras.layers.GRU(32,return_sequences=True),\n",
    "      keras.layers.Dense(s.size)\n",
    "])\n",
    "f_s_rnn.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# option2: every state + every action => return next state\n",
    "# state model (state, action) -> state\n",
    "\n",
    "f_s = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=np.vstack([s, a]).reshape(-1).shape),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(32),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(s.size)#,\n",
    "    # keras.layers.Activation('tanh')\n",
    "])\n",
    "f_s.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%% reward model (state, action) -> reward\n"
    }
   },
   "outputs": [],
   "source": [
    "f_r = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=np.vstack([s, a]).reshape(-1).shape),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(32),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(1)#,\n",
    "    # keras.layers.Activation('tanh')\n",
    "])\n",
    "f_r.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Policy Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%% policy model (state) -> action\n"
    }
   },
   "outputs": [],
   "source": [
    "pi = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=s.reshape(-1).shape),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(32),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(a.size)#,\n",
    "    # keras.layers.Activation('tanh')\n",
    "])\n",
    "pi.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random exploration policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "pi_explore = lambda state : np.random.randint(low=a_min, high=a_max, size=ndim)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% exploration policy (state) -> action\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observe Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% observe environment dynamics using pi_explore\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "record_r = []\n",
    "record_a = []\n",
    "record_s = []\n",
    "record_sequence_a = []\n",
    "record_sequence_s = []\n",
    "\n",
    "# generate M random trajectories of length T\n",
    "trajectories = np.zeros(shape=(M, T, ndim), dtype=np.float)\n",
    "for m in range(M):\n",
    "    for t in range(1, T):\n",
    "        trajectories[m, t] = trajectories[m, t-1] + np.random.randint(low=d_min, high=d_max, size=ndim, dtype='l')\n",
    "\n",
    "validation_trajectories = np.zeros(shape=(M, T, ndim), dtype=np.float)\n",
    "for m in range(M):\n",
    "    for t in range(1, T):\n",
    "        validation_trajectories[m, t] = validation_trajectories[m, t-1] + np.random.randint(low=d_min, high=d_max, size=ndim, dtype='l')\n",
    "\n",
    "\n",
    "num_episodes = 15 # explore each trajectory a few times\n",
    "for _ in range(num_episodes):\n",
    "\n",
    "    # for each trajectory\n",
    "    for m in range(trajectories.shape[0]):\n",
    "\n",
    "        # reset agent's state:\n",
    "        s[:] = np.zeros_like(s)\n",
    "\n",
    "        tmp_sequence_s = []\n",
    "        tmp_sequence_a = []\n",
    "\n",
    "        # for up to maximum time steps T:\n",
    "        #for t in range(T): # todo: instead of maximum steps => try as long until a certain length is reached\n",
    "        t = 0\n",
    "        while t < T:\n",
    "\n",
    "            # observe\n",
    "            y[:min(num_observables, T - t)] = trajectories[m, t : min(t + num_observables, T)]\n",
    "\n",
    "            # get action\n",
    "            a = pi_explore(s)\n",
    "\n",
    "            # update state\n",
    "            v_ = velocity_next(v, a)\n",
    "            x_ = position_next(x, v)\n",
    "            # v[:] = velocity_next(v, a)\n",
    "            # x[:] = position_next(x, v)\n",
    "\n",
    "            # reward\n",
    "            r = get_reward(x, y[0])\n",
    "\n",
    "            # cancel episode if agent gets too far away from his target\n",
    "            if np.linalg.norm(x_ - y[0]) > max_agent_distance:\n",
    "                continue\n",
    "\n",
    "            v[:] = v_\n",
    "            x[:] = x_\n",
    "\n",
    "            # store observation\n",
    "            record_s.append(s.copy())\n",
    "            record_a.append(a.copy())\n",
    "            record_r.append(r.copy())\n",
    "            tmp_sequence_s.append(s.copy())\n",
    "            tmp_sequence_a.append(a.copy())\n",
    "\n",
    "            t += 1\n",
    "\n",
    "        record_sequence_s.append(np.array(tmp_sequence_s.copy()))\n",
    "        record_sequence_a.append(np.array(tmp_sequence_a.copy()))\n",
    "\n",
    "record_s = np.array(record_s)\n",
    "record_a = np.array(record_a)\n",
    "record_r = np.array(record_r)\n",
    "# record_sequence_s = np.array(record_sequence_s)\n",
    "# record_sequence_a = np.array(record_sequence_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# np.vstack(record_sequence_s)[0]\n",
    "# np.array(record_sequence_s).shape\n",
    "record_sequence_a[0].shape\n",
    "# trajectories[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Normalize Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% normalize data\n"
    }
   },
   "outputs": [],
   "source": [
    "# copy state:\n",
    "s_norm = record_s.copy()                            # x, v, y: agent's position is always 0/0, target is normed to max_distance\n",
    "x_norm = s_norm[0,:]                                # position\n",
    "v_norm = s_norm[1,:]                                # velocity\n",
    "y_norm = s_norm[2:,:]                               # target(s)\n",
    "\n",
    "# normalize state and make next target relative to agent's position:\n",
    "v_norm[:] = v_norm / v_max                          # normed to maximum velocity\n",
    "y_norm[:] = (y_norm - x_norm) / max_agent_distance  # relative to agent's position and normed to maximum distance from agent to next target\n",
    "x_norm[:] = np.zeros_like(x_norm)                   # agent is always at the center\n",
    "\n",
    "# normalize action:\n",
    "a_norm = record_a / a_max                           # normed to maximum acceleration\n",
    "\n",
    "# normalize reward:\n",
    "r_norm = record_r / -max_agent_distance             # normed to negative maximum reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% train reward model\n"
    },
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "state_action = np.concatenate([s_norm, np.expand_dims(a_norm, 1)], axis=1).reshape(s_norm.shape[0], -1)\n",
    "f_r.fit(state_action, r_norm, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train State Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% train state model\n"
    }
   },
   "outputs": [],
   "source": [
    "# state_action2 = np.concatenate([s_norm, np.expand_dims(a_norm, 1)], axis=1).reshape(s_norm.shape[0], -1)[:-1]\n",
    "# f_s.fit(state_action2, s_norm.reshape(s_norm.shape[0], -1)[1:], batch_size, epochs)\n",
    "\n",
    "# state_action2 = np.concatenate([s_norm, np.expand_dims(a_norm, 1)], axis=1).reshape(s_norm.shape[0], -1)[:-1]\n",
    "# state_action2 = a_norm.reshape(batch_size, timesequence, input_dim)\n",
    "timesequence = 500\n",
    "input_dim = a_norm.shape[-1]\n",
    "state_action2 = a_norm.reshape(timesequence, ndim)\n",
    "f_s_rnn.fit(state_action2, s_norm, batch_size, epochs)\n",
    "# f_s_rnn.fit(state_action2, )\n",
    "# get a sequence of actions => output sequence of states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare real and learned environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% validate trained model to real environment\n"
    }
   },
   "outputs": [],
   "source": [
    "# for each trajectory\n",
    "for m in range(trajectories.shape[0]):\n",
    "\n",
    "    # reset agent's state:\n",
    "    s[:] = np.zeros_like(s)\n",
    "\n",
    "    # for up to maximum time steps T:\n",
    "    for t in range(T):\n",
    "\n",
    "        # observe\n",
    "        y[:min(num_observables, T - t)] = trajectories[m, t : min(t + num_observables, T)]\n",
    "\n",
    "        # get action\n",
    "        a = pi_explore(s)\n",
    "        \n",
    "        # update state\n",
    "        v[:] = velocity_next(v, a)\n",
    "        x[:] = position_next(x, v)\n",
    "        # v_norm = f_s(np.concatenate([s_norm, np.expand_dims(a_norm, 1)], axis=1).reshape(s_norm.shape[0], -1)[:-1])\n",
    "\n",
    "        # reward\n",
    "        r = get_reward(x, y[0])\n",
    "\n",
    "        # cancel episode if agent gets too far away from his target\n",
    "        if np.linalg.norm(x - y[0]) > max_agent_distance:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Model-Based-Reinforcement-Learning)",
   "language": "python",
   "name": "pycharm-5fde42b4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}