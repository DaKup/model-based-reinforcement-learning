{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%% imports\n"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorboard\n"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%% settings\n"
    }
   },
   "outputs": [],
   "source": [
    "ndim            = 2         # number of dimensions\n",
    "T               = 500       # maximum time steps := length of each trajectory\n",
    "M               = 250       # number of generated trajectories from observations of pi_explore\n",
    "a_max           = 100       # maximum acceleration\n",
    "d_max           = 3 * a_max # max distance between trajectory points\n",
    "v_max           = 5 * a_max # maximum velocity\n",
    "a_min           = -a_max    # minimum acceleration\n",
    "v_min           = -v_max    # minimum velocity\n",
    "d_min           = 1         # min distance between trajectory points\n",
    "num_observables = 3         # observable targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "a_v = 2 # constant\n",
    "a_x = 2 # constant\n",
    "velocity_next   = lambda velocity, acceleration : velocity + a_v * acceleration\n",
    "position_next   = lambda position, velocity     : position + a_x * velocity\n",
    "get_reward      = lambda position, target       : -np.dot((position - target), np.transpose(position - target))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% toy environment dynamics\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "a = np.zeros(shape=ndim)        # current acceleration\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% action space\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "s = np.zeros(shape=(2+num_observables, ndim))   # current state\n",
    "x = s[0,:]                      # position\n",
    "v = s[1,:]                      # velocity\n",
    "y = s[2:,:]                     # target(s)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% state space\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "pi_explore = lambda state : np.random.randint(low=a_min, high=a_max, size=ndim, dtype='l')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% exploration policy (state) -> action\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "pi = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=s.shape),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(a.size),\n",
    "    keras.layers.Activation('tanh')\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% policy model (state) -> action\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "f_s = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=np.vstack([s, a]).shape),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(s.size),\n",
    "    keras.layers.Activation('tanh')\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% state model (state, action) -> state\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "f_r = keras.Sequential([\n",
    "    keras.layers.Dense(32, input_shape=np.vstack([s, a]).shape),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(1),\n",
    "    keras.layers.Activation('tanh')\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% reward model (state, action) -> reward\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "trajectories = np.zeros(shape=(M, T, ndim), dtype=np.float)\n",
    "for m in range(M):\n",
    "    for t in range(1, T):\n",
    "        trajectories[m, t] = trajectories[m, t-1] + np.random.randint(low=d_min, high=d_max, size=ndim, dtype='l')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% generate trajectories (should be done using pi_explore)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "num_episodes = 1\n",
    "replay_buffer = []\n",
    "for _ in range(num_episodes):\n",
    "\n",
    "    # for each trajectory (or perhaps create mini batches # => no minibatches, because trajectories may end at different times?)\n",
    "    for m in range(M):\n",
    "\n",
    "        # reset:\n",
    "        x[:] = np.zeros_like(x)\n",
    "        v[:] = np.zeros_like(v)\n",
    "\n",
    "        # for each step:\n",
    "        for t in range(T):\n",
    "\n",
    "            # get state\n",
    "            tmp = min(num_observables, T - t)\n",
    "            y[:tmp] = trajectories[m, t : min(t + num_observables, T)]\n",
    "\n",
    "            # get action\n",
    "            a = pi_explore(s)\n",
    "\n",
    "            # perform update\n",
    "            v[:] = velocity_next(v, a)\n",
    "            x[:] = position_next(x, v)\n",
    "\n",
    "            # get reward\n",
    "            r = get_reward(x, y[0])\n",
    "            replay_buffer.append((s, a, r))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% explore and observe real environment\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# f_s = None\n",
    "# f_r = None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% train world model\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pi = None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% train policy\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% evaluate\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-5fde42b4",
   "language": "python",
   "display_name": "PyCharm (Model-Based-Reinforcement-Learning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}